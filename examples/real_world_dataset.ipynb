{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fast Gradient Algorithm Coefficients:\n",
      " [ 0.02117345 -0.03877587  0.09773771  0.05503818  0.15256401  0.13534365\n",
      "  0.28352681  0.15114461  0.11280873  0.06183422  0.10502022 -0.04219962\n",
      "  0.03472887  0.02859174  0.10917878  0.27363777  0.17373109  0.12517347\n",
      "  0.1290986   0.12243222  0.22405219  0.10833277  0.24549208  0.16655949\n",
      " -0.14754037 -0.10493049 -0.1149898  -0.06087818 -0.04741795 -0.06626914\n",
      " -0.02882511 -0.0151532  -0.07621369 -0.01594251 -0.04245255 -0.01357162\n",
      " -0.07700499 -0.03415877 -0.07954034  0.01733619 -0.04772963 -0.09082758\n",
      " -0.0600434  -0.06969745 -0.11271605 -0.11263913 -0.0321344  -0.06206727\n",
      " -0.05727232 -0.0425365  -0.02823825  0.15484338  0.26205746  0.05893875\n",
      "  0.068719    0.12509357  0.14818851]\n",
      "\n",
      "Sci-kit Learn's LogisticRegression() Coefficients:\n",
      " [[ 0.02117345 -0.03877587  0.09773771  0.05503819  0.15256401  0.13534366\n",
      "   0.28352681  0.15114461  0.11280873  0.06183421  0.10502022 -0.04219962\n",
      "   0.03472887  0.02859174  0.10917878  0.27363777  0.17373109  0.12517347\n",
      "   0.1290986   0.12243222  0.2240522   0.10833277  0.24549208  0.16655949\n",
      "  -0.14754037 -0.10493049 -0.1149898  -0.06087818 -0.04741795 -0.06626914\n",
      "  -0.02882511 -0.0151532  -0.07621369 -0.0159425  -0.04245255 -0.01357162\n",
      "  -0.07700499 -0.03415877 -0.07954035  0.01733619 -0.04772964 -0.09082759\n",
      "  -0.06004341 -0.06969745 -0.11271606 -0.11263913 -0.0321344  -0.06206727\n",
      "  -0.05727232 -0.0425365  -0.02823825  0.15484338  0.26205747  0.05893876\n",
      "   0.06871901  0.12509356  0.14818851]]\n",
      "\n",
      "Misclassification Error: 9.44%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code shows an example implementing the fast gradient algorithm to compute\n",
    "the beta coefficients and misclassification error. It compares the beta\n",
    "coefficients to that of sklearn.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import src.logistic_reg as lreg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# load dataset and drop NAs\n",
    "spam = pd.read_table('https://statweb.stanford.edu/~tibs/ElemStatLearn/'\n",
    "                     'datasets/spam.data', sep=' ', header=None)\n",
    "spam = spam.dropna()\n",
    "test_indicator = pd.read_table('https://statweb.stanford.edu/~tibs/'\n",
    "                               'ElemStatLearn/datasets/spam.traintest',\n",
    "                               sep=' ', header=None)\n",
    "\n",
    "# declare data and labels\n",
    "x_data = np.asarray(spam.drop(57, axis=1))\n",
    "y_data = np.asarray(spam[57])*2 - 1\n",
    "test_indicator = np.ravel(np.asarray(test_indicator))\n",
    "\n",
    "# define the split between train and test data\n",
    "x_train = x_data[test_indicator == 0, :]\n",
    "x_test = x_data[test_indicator == 1, :]\n",
    "y_train = y_data[test_indicator == 0]\n",
    "y_test = y_data[test_indicator == 1]\n",
    "\n",
    "# standardize the data\n",
    "x_scaler = StandardScaler().fit(x_train)\n",
    "x_train = x_scaler.transform(x_train)\n",
    "x_test = x_scaler.transform(x_test)\n",
    "n = x_train.shape[0]\n",
    "d = x_train.shape[1]\n",
    "\n",
    "# initialize the beta and theta values\n",
    "beta_init = np.zeros(d)\n",
    "theta_init = np.zeros(d)\n",
    "\n",
    "# run the fast gradient algorithm to find the beta coefficients\n",
    "fastgrad_betas = lreg.fastgradalgo(beta_init=beta_init,\n",
    "                                   theta_init=theta_init,\n",
    "                                   lamb=0.1,\n",
    "                                   x=x_train,\n",
    "                                   y=y_train,\n",
    "                                   max_iter=1000)[-1]\n",
    "\n",
    "# run sci-kit learn's LogisticRegression() to find the beta coefficients\n",
    "logit = LogisticRegression(C=1/(2*n*0.1),\n",
    "                           fit_intercept=False,\n",
    "                           tol=1e-8).fit(x_train, y_train)\n",
    "\n",
    "# print the coefficients found using the fast gradient algorithm and sklearn\n",
    "print(\"\\nFast Gradient Algorithm Coefficients:\\n\", fastgrad_betas)\n",
    "print(\"\\nSci-kit Learn's LogisticRegression() Coefficients:\\n\", logit.coef_)\n",
    "\n",
    "# apply the coefficients found using the fast gradient algorithm to test set\n",
    "y_predict = (np.dot(x_test, fastgrad_betas) > 0)*2 - 1\n",
    "\n",
    "# print the misclassification error\n",
    "print(\"\\nMisclassification Error: %.2f%%\" % (np.mean(y_predict != y_test)*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
