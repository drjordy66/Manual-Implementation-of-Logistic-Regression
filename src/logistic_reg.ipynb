{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logistic_reg_fastgrad\n",
    "=====================\n",
    "\n",
    "This code implements the fast gradient algorithm to solve the following $\\ell_2^2$-regularized logistic regression problem:\n",
    "\n",
    "$$\\min_{\\beta\\epsilon\\mathbb{R}}F(\\beta):=\\frac{1}{n}\\sum_{i=1}^nlog\\left(1+\\exp(-y_ix_i^T\\beta)\\right)+\\lambda\\lvert\\lvert\\beta\\rvert\\rvert_2^2$$\n",
    "\n",
    "See the [examples](https://github.com/drjordy66/logistic_reg_fastgrad/tree/master/examples) section on how to use this algorithm on both a real-world dataset and a simulated dataset for classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimizing Logistic Regression using Fast Gradient Algorithm with Backtracking\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def computeobj(beta, lamb, x, y):\n",
    "    \"\"\"\n",
    "    Compute a return a single value for the objective function\n",
    "    :param beta: array\n",
    "        initialize betas (typically zeros)\n",
    "    :param lamb: float\n",
    "        regularization parameter\n",
    "    :param x: array\n",
    "        data\n",
    "    :param y: array\n",
    "        labels\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    obj = (1/n)*(np.sum(np.log(1 + np.exp(-y*np.dot(x, beta))))) \\\n",
    "        + lamb*np.sum(beta**2)\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def computegrad(beta, lamb, x, y):\n",
    "    \"\"\"\n",
    "    Compute the gradient of the objective function and return a vector size d\n",
    "    :param beta: array\n",
    "        initialize betas (typically zeros)\n",
    "    :param lamb: float\n",
    "        regularization parameter\n",
    "    :param x: array\n",
    "        data\n",
    "    :param y: array\n",
    "        labels\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "\n",
    "    grad_beta = -(1/n)*(np.dot(x.T, y/(np.exp(y*np.dot(x, beta)) + 1))) \\\n",
    "        + 2*lamb*beta\n",
    "\n",
    "    return grad_beta\n",
    "\n",
    "\n",
    "def backtracking(beta, lamb, x, y, eta=1, alpha=0.5, gamma=0.8, max_iter=100):\n",
    "    \"\"\"\n",
    "    :param beta: array\n",
    "        initialize betas (typically zeros)\n",
    "    :param lamb: float\n",
    "        regularization parameter\n",
    "    :param x: array\n",
    "        data\n",
    "    :param y: array\n",
    "        labels\n",
    "    :param eta: float\n",
    "        initial step-size for backtracking\n",
    "    :param alpha: float\n",
    "        constant for sufficient decrease condition\n",
    "    :param gamma: float\n",
    "        constant for sufficient decrease condition\n",
    "    :param max_iter: int\n",
    "        stopping criterion\n",
    "    \"\"\"\n",
    "\n",
    "    grad_beta = computegrad(beta, lamb, x, y)\n",
    "    norm_grad_beta = np.sqrt(np.sum(grad_beta**2))\n",
    "    found_eta = 0\n",
    "    t = 0\n",
    "\n",
    "    while found_eta == 0 and t < max_iter:\n",
    "        if (computeobj(beta - eta*grad_beta, lamb, x, y) <\n",
    "                computeobj(beta, lamb, x, y) - alpha*eta*norm_grad_beta**2):\n",
    "            found_eta = 1\n",
    "        elif t == max_iter:\n",
    "            break\n",
    "        else:\n",
    "            eta = eta*gamma\n",
    "            t += 1\n",
    "\n",
    "    return eta\n",
    "\n",
    "\n",
    "def fastgradalgo(beta_init, theta_init, lamb, x, y, max_iter, eps=1e-5):\n",
    "    \"\"\"\n",
    "    :param beta_init: array\n",
    "        initialize betas (typically zeros)\n",
    "    :param theta_init: array\n",
    "        initialize thetas (typically zeros)\n",
    "    :param lamb: float\n",
    "        regularization parameter\n",
    "    :param x: array\n",
    "        data\n",
    "    :param y: array\n",
    "        labels\n",
    "    :param max_iter: int\n",
    "        stopping criterion\n",
    "    \"\"\"\n",
    "\n",
    "    n = x.shape[0]\n",
    "    beta = beta_init\n",
    "    theta = theta_init\n",
    "    grad_theta = computegrad(theta, lamb, x, y)\n",
    "    eta_init = 1/(max(np.linalg.eigh(np.dot((1/n)*x.T, x))[0]) + lamb)\n",
    "    beta_vals = [beta_init]\n",
    "    t = 0\n",
    "\n",
    "    while t < max_iter and np.linalg.norm(grad_theta) > eps:\n",
    "        eta = backtracking(beta, lamb, x, y, eta=eta_init)\n",
    "        beta_next = theta - eta*grad_theta\n",
    "        theta = beta_next + t*(beta_next - beta)/(t + 3)\n",
    "        grad_theta = computegrad(theta, lamb, x, y)\n",
    "        beta = beta_next\n",
    "        beta_vals.append(beta)\n",
    "        t += 1\n",
    "\n",
    "    return beta_vals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
